{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcc0354",
   "metadata": {},
   "source": [
    "# üßô Gandalf Chatbot (Fully Local)\n",
    "This notebook runs a LangChain RAG chatbot locally using `transformers`, `sentence-transformers`, and `FAISS`. No cloud APIs or internet needed after model download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f995ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install Required Libraries\n",
    "!pip install -q transformers sentence-transformers langchain faiss-cpu pypdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîê Load Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40606bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Load and Split PDF\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"lotr_book.pdf\")  # Change to your PDF path\n",
    "pages = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(pages)\n",
    "print(f\"‚úÖ Loaded and split {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a70c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîé Embed Chunks and Save Vectorstore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"gandalf_index\")\n",
    "print(\"‚úÖ Vectorstore saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e502da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Load Local LLM with Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Or other instruct-tuned LLM\n",
    "\n",
    "print(\"‚è¨ Loading model (first time may take a few minutes)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0.7)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"‚úÖ LLM loaded locally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e64f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Load Vectorstore and Run RAG\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = FAISS.load_local(\"gandalf_index\", embeddings).as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "question = \"What happened in the mines of Moria?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(\"üßô Gandalf says:\\n\", result['result'])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
